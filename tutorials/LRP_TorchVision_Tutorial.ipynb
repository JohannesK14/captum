{
  "cells": [
    {
      "source": [
        "# LRP Tutorial for Pretrained VGG16 Model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "This notebook demonstrates how to apply the Layer-Wise Relevance Propagation (LRP) algorithm on a pre-trained VGG16 model using a sample image. The relevance of each pixel is visualized by overlaying them on the example image. Further details regarding the operating principles of LRP can be found at [heatmapping.org](http://heatmapping.org/) and [here](https://www.springerprofessional.de/layer-wise-relevance-propagation-an-overview/17153814).\n",
        "\n",
        "The tutorial uses the same sample image and rule configuration as in [this](https://git.tu-berlin.de/gmontavon/lrp-tutorial) PyTorch implementation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note: Before running this tutorial, please install the torchvision and PIL packages.\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from captum.attr import LRP\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr._utils.lrp_rules import EpsilonRule, GammaRule"
      ]
    },
    {
      "source": [
        "Loads the sample image and performs the appropriate normalizing steps."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "img = Image.open('img/lrp/castle.jpg')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),  # needed for application of ResNet, VGGNet, ...\n",
        "    ]\n",
        ")\n",
        "\n",
        "X = torch.unsqueeze(transform(img), 0)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "Loads pre-trained VGG16 model and sets it to eval mode."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "source": [
        "Direct generation of LRP attribution. The default Epsilon-Rule is used for every layer. As one see in the generated output image, this does not bring many new insights as the heatmap does not focus well on the image content."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "lrp = LRP(model)\n",
        "attribution = lrp.attribute(X, target=483, verbose=True)  # castle -> 483\n",
        "attribution = attribution.squeeze().permute(1, 2, 0).detach().numpy()\n",
        "\n",
        "_ = viz.visualize_image_attr(\n",
        "    attribution,\n",
        "    img,\n",
        "    method='blended_heat_map',\n",
        "    sign=all,\n",
        "    show_colorbar=True,\n",
        "    title='Overlayed LRP',\n",
        ")"
      ]
    },
    {
      "source": [
        "But one can assign different rules to every layer. This is a crucial step to get expressive heatmaps. In the literature, one can find recommendations on when to use which layer. Currently implemented in captum are LRP-Epsilon, LRP-0, LRP-Gamma, LRP-Alpha-Beta, and the Identity-Rule.\n",
        "\n",
        "In the next steps, a list of all layers is generated and a rule is assigned to each one."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = list(model._modules['features']) + list(model._modules['classifier'])\n",
        "number_layers = len(layers)\n",
        "\n",
        "for idx_layer in range(1, number_layers)[::-1]:\n",
        "    if idx_layer <= 16:\n",
        "        setattr(layers[idx_layer], 'rule', GammaRule())\n",
        "    if 17 <= idx_layer <= 30:\n",
        "        setattr(layers[idx_layer], 'rule', EpsilonRule())\n",
        "    if idx_layer >= 31:\n",
        "        setattr(layers[idx_layer], 'rule', EpsilonRule(epsilon=0))  # LRP-0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "lrp = LRP(model)\n",
        "attribution = lrp.attribute(X, target=483, verbose=True)  # castle -> 483\n",
        "attribution = attribution.squeeze().permute(1, 2, 0).detach().numpy()\n",
        "\n",
        "_ = viz.visualize_image_attr(\n",
        "    attribution,\n",
        "    img,\n",
        "    method='blended_heat_map',\n",
        "    sign='all',\n",
        "    show_colorbar=True,\n",
        "    title='Overlayed LRP',\n",
        ")"
      ]
    },
    {
      "source": [
        "With the verbose parameter, one can check the correct application of the rules in the generated output. As one can see in the generated output image, the heatmap shows clearly positive attributions for the silhouette of the castle. In contrast, the road traffic sign and the lantern are contributing negatively to the class 'castle'."
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
